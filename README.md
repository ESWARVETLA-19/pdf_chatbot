‚öôÔ∏è Setup & Usage
1Ô∏è‚É£ Clone Repository
```
git clone https://github.com/ESWARVETLA-19/pdf_chatbot.git
cd pdf_chatbot
```
2Ô∏è‚É£ Add PDFs

Place your PDF files in the pdfs/ folder.

3Ô∏è‚É£ Build Vector DB
```docker-compose run app python app/ingest.py```

4Ô∏è‚É£ Start Services
```docker-compose up```


This will start:

Ollama service (LLM runtime)

App service (RAG pipeline + streamlit)

5Ô∏è‚É£ Chat with PDFs

Access the app in your browser (depends on Streamlit/React config).
Ask questions and get contextual answers from your documents.



**üß© How It Works**

PDF Ingestion ‚Üí Extract text ‚Üí Split into chunks.

Embedding ‚Üí Convert text chunks into vectors using HuggingFace MiniLM.

Storage ‚Üí Save embeddings into ChromaDB (persistent).

Retrieval ‚Üí At query time, fetch the most relevant chunks.

Generation ‚Üí Mistral LLM (via Ollama) generates answers using retrieved context.
